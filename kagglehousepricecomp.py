# -*- coding: utf-8 -*-
"""kagglehousepricecomp.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1XFTwC4ipTZIfQ2rcgqB9lcKlUy65GNRU
"""

import pandas as pd
from google.colab import drive

# 1) Mount your Google Drive
drive.mount('/content/drive')

# 2) Define file paths
file_path_train = '/content/drive/My Drive/House_Price_Comp/df_train_cleaned.csv'
file_path_test  = '/content/drive/My Drive/House_Price_Comp/df_test_cleaned.csv'

# 3) Read CSVs into DataFrames
df_train = pd.read_csv(file_path_train)
df_test  = pd.read_csv(file_path_test)

# 4) (Optional) Quick sanity check
print(f"Training set: {df_train.shape[0]} rows, {df_train.shape[1]} columns")
print(f"Test set:     {df_test.shape[0]} rows, {df_test.shape[1]} columns")

feature_onehot = ['MSZoning','Street','Alley','LotConfig',
                  'Condition1','Condition2','BldgType','HouseStyle',
                  'RoofStyle','RoofMatl','Exterior1st','Exterior2nd',
                  'MasVnrType','Foundation','Heating',
                  'Electrical','GarageType',
                  'MiscFeature','SaleType','SaleCondition'
                  ]
ordinal_2 = [
    'LotShape', 'LandContour', 'LandSlope', 'BsmtExposure',
    'BsmtFinType1', 'BsmtFinType2', 'Utilities', 'CentralAir',
    'Functional', 'GarageFinish', 'PavedDrive', 'Fence'
]
feature_ordinal = ['ExterQual','ExterCond','BsmtQual','BsmtCond',
                   'HeatingQC','KitchenQual','FireplaceQu','GarageQual','GarageCond']

feature_numeric = ['LotFrontage','LotArea','OverallQual','OverallCond','Age','remodAge','MiscVal',
                   'MasVnrArea', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF',
                    '1stFlrSF', '2ndFlrSF', 'LowQualFinSF', 'GrLivArea',
                   'TotRmsAbvGrd','Fireplaces','GarageCars','GarageArea',
                   'WoodDeckSF', 'OpenPorchSF','EnclosedPorch','3SsnPorch','ScreenPorch','PoolArea',
                   'MiscVal', 'Neighborhood_TE','TotalBaths','BsmtFullBath',
                   'BsmtHalfBath','HalfBath','FullBath']

df_test['TotalBaths'] = df_test['BsmtFullBath'] + df_test['BsmtHalfBath'] + df_test['FullBath'] + df_test['HalfBath']
df_train['TotalBaths'] = df_train['BsmtFullBath'] + df_test['BsmtHalfBath'] + df_test['FullBath'] + df_test['HalfBath']
# Compute the mean SalePrice for each Neighborhood in the training set
neighborhood_means = df_train.groupby('Neighborhood')['SalePrice'].mean()

# Compute the global mean SalePrice (for unseen neighborhoods in test)
global_mean = df_train['SalePrice'].mean()

# Map the neighborhood means to the training set
df_train['Neighborhood_TE'] = df_train['Neighborhood'].map(neighborhood_means)

# Map the same training‐set means to the test set, filling unknowns with the global mean
df_test['Neighborhood_TE'] = (
    df_test['Neighborhood']
      .map(neighborhood_means)
      .fillna(global_mean)
)
#data cleaning
import numpy as np
import numpy as np

cols_to_drop = cols_to_drop = ['MoSold','YrSold', 'YearBuilt', 'GarageYrBlt', 'YearRemodAdd','Neighborhood','PoolQC']

for df in (df_train, df_test):
    df.drop(columns=cols_to_drop, inplace=True, errors='ignore')
# feature_ordinal should be defined elsewhere in your script, e.g.:
# feature_ordinal = ['BsmtQual', 'BsmtCond', 'ExterQual', 'ExterCond', ...]

# Loop over each ordinal feature and replace missing values with the string 'None'
for col in ordinal_2:
    df_train[col] = df_train[col].replace({np.nan: 'None', None: 'None'})
    df_test[col] = df_test[col].replace({np.nan: 'None', None: 'None'})
    # Verify the replacement for each column
    print(f"Column: {col}")
    print(df_train[col].unique())
    print(df_train[col].value_counts(dropna=False))
    print()


# Loop over each ordinal feature and replace missing values with the string 'None'
for col in feature_ordinal:
    df_train[col] = df_train[col].replace({np.nan: 'None', None: 'None'})
    df_test[col] = df_test[col].replace({np.nan: 'None', None: 'None'})
    # Verify the replacement for each column
    print(f"Column: {col}")
    print(df_train[col].unique())
    print(df_train[col].value_counts(dropna=False))
    print()

for col in feature_onehot:
    df_train[col] = df_train[col].replace({np.nan: 'None', None: 'None'})
    df_test[col] = df_test[col].replace({np.nan: 'None', None: 'None'})
    # Verify the replacement for each column
    print(f"Column: {col}")
    print(df_train[col].unique())
    print(df_train[col].value_counts(dropna=False))
    print()

#df_train[replace] = df[replace].fillna('None')
for col in feature_numeric:
    # Fill NaNs with 0, clip any negative values to 0, then convert to int
    df_train[col] = df_train[col].fillna(0).clip(lower=0).astype(int)
    df_test[col]  = df_test[col].fillna(0).clip(lower=0).astype(int)

import seaborn as sns
import matplotlib.pyplot as plt
test_cor = df_train.select_dtypes(include='number').corr().round(2)
test_cor

sns.heatmap(test_cor,annot=True)
plt.figure(figsize=(20, 20))  # adjust width and height as needed
sns.heatmap(test_cor, annot=True,cmap='coolwarm')
plt.show()

sns.pairplot(df_train,vars=['Neighborhood_TE','SquareFeet','OverallQual'])
plt.show()

sns.histplot(df_train["SalePrice"],bins = 40, kde=False)
plt.title('SalePrice Ditribution')
plt.show()

#one hot enoding
import pandas as pd

feature_onehot = ['MSZoning','Street','Alley','LotConfig',
                  'Condition1','Condition2','BldgType','HouseStyle',
                  'RoofStyle','RoofMatl','Exterior1st','Exterior2nd',
                  'MasVnrType','Foundation','Heating',
                  'Electrical','GarageType',
                  'MiscFeature','SaleType','SaleCondition'
                  ]
# One‐hot encode df_train
df_train = pd.get_dummies(
    df_train,
    columns=feature_onehot,
    prefix_sep='__',
    drop_first=False
)

# One‐hot encode df_test
df_test = pd.get_dummies(
    df_test,
    columns=feature_onehot,
    prefix_sep='__',
    drop_first=False
)

# Label encode all the ordinal data individually, and add to list

ordinal_2 = [
    'LotShape', 'LandContour', 'LandSlope', 'BsmtExposure',
    'BsmtFinType1', 'BsmtFinType2', 'Utilities', 'CentralAir',
    'Functional', 'GarageFinish', 'PavedDrive', 'Fence'
]
for df in (df_train, df_test):
    df['LotShape']= df['LotShape'].map({"None": 0, "IR3": 1, "IR2": 2, "IR1": 3, "Reg": 4}).astype(int)
    df['LandContour']= df['LandContour'].map({"None": 0, "Low": 1, "Bnk": 2, "HLS": 3, "Lvl": 4}).astype(int)
    df['LandSlope']  = df['LandSlope'].map({"None": 0, "Sev": 1, "Mod": 2, "Gtl": 3}).astype(int)
    df['BsmtExposure']  = df['BsmtExposure'].map({"None": 0, "No": 1, "Mn": 2, "Av": 3, "Gd": 4}).astype(int)
    df['BsmtFinType1']  = df['BsmtFinType1'].map({"None": 0, "Unf": 1, "LwQ": 2, "Rec": 3, "BLQ": 4, "ALQ": 5, "GLQ": 6}).astype(int)
    df['BsmtFinType2']  = df['BsmtFinType2'].map({"None": 0, "Unf": 1, "LwQ": 2, "Rec": 3, "BLQ": 4, "ALQ": 5, "GLQ": 6}).astype(int)
    df['Utilities'] = df['Utilities'].map({"None": 0, "ELO": 1, "NoSeWa": 2, "NoSewr": 3, "AllPub": 4}).astype(int)
    df['CentralAir'] = df['CentralAir'].map({"None": 0, "N": 1, "Y": 2}).astype(int)
    df['Functional'] = df['Functional'].map({"None": 0, "Sal": 1, "Sev": 2, "Maj2": 3, "Maj1": 4,"Mod": 5, "Min2": 6, "Min1": 7, "Typ": 8}).astype(int)
    df['GarageFinish']  = df['GarageFinish'].map({"None": 0, "Unf": 1, "RFn": 2, "Fin": 3}).astype(int)
    df['PavedDrive']  = df['PavedDrive'].map({"None": 0, "N": 1, "P": 2, "Y": 3}).astype(int)
    df['Fence']  = df['Fence'].map({"None": 0, "MnWw": 1, "GdWo": 2, "MnPrv": 3, "GdPrv": 4}).astype(int)



# Align test to train columns
df_test = df_test.reindex(columns=df_train.columns, fill_value=0)
#general ordinal encoding
import numpy as np

# List of ordinal features
feature_ordinal = ['ExterQual','ExterCond','BsmtQual','BsmtCond',
                   'HeatingQC','KitchenQual','FireplaceQu','GarageQual','GarageCond']
# Define the mapping
ordinal_map = {
    'None': 0,
    'Po':   1,
    'Fa':   2,
    'TA':   3,
    'Gd':   4,
    'Ex':   5
}

for col in feature_ordinal:
    # 1) unify missing values as the string 'None'
    df_train[col] = df_train[col].replace({np.nan: 'None', None: 'None'})
    df_test[col]  = df_test[col].replace({np.nan: 'None', None: 'None'})

    # 2) map to ordinal integers
    df_train[col] = df_train[col].map(ordinal_map).astype(int)
    df_test[col]  = df_test[col].map(ordinal_map).astype(int)

import scipy.stats
numerical_cols = feature_numeric

skew_df = pd.DataFrame(numerical_cols,columns=['Feature'])

skew_df['Skew'] = skew_df['Feature'].apply(lambda feature: scipy.stats.skew(df_train[feature]))

skew_df['Absolute Skew'] = skew_df['Skew'].apply(abs)

skew_df["Skewed"] = skew_df['Absolute Skew'].apply(lambda x: True if x >= 0.5 else False)

skew_df

for column in skew_df[skew_df['Skewed'] == True]['Feature']:
    df_train[column] = np.log1p(df_train[column])
    df_test[column] = np.log1p(df_test[column])
import numpy as np

# Apply a log(1 + x) transform to the SalePrice column in your training set
df_train['SalePrice'] = np.log(df_train['SalePrice'])

# (Optional) Verify the transformation
print("Original min/max:", df_train['SalePrice'].min(), df_train['SalePrice'].max())
print("Skew after log1p:", df_train['SalePrice'].skew())

import numpy as np
import pandas as pd
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score, mean_squared_log_error

# 1) Feature matrix: use every column except 'SalePrice' (already log‐transformed) and 'Id'
features  = [c for c in df_train.columns if c not in ('SalePrice','Id')]

# 2) Prepare X, y and clean non‐finite values
X = (
    df_train[features]
      .replace([np.inf, -np.inf], np.nan)
      .fillna(0)
)
y = df_train['SalePrice']   # assume this is already log1p(original SalePrice)

X_test = (
    df_test[features]
      .replace([np.inf, -np.inf], np.nan)
      .fillna(0)
)

# 3) Train/validation split
X_train, X_val, y_train, y_val = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# 4) Instantiate and fit a Gradient Boosting Regressor
gbr = GradientBoostingRegressor(
    n_estimators=1005,
    learning_rate=0.021,
    max_depth=5,
    subsample=0.5,
    max_features='sqrt',
    random_state=42
)
gbr.fit(X_train, y_train)

# 5) Predict on the validation set (log‐scale)
y_val_pred_log = gbr.predict(X_val)

# 6) Metrics on the log‐scale
rmse_log = np.sqrt(mean_squared_error(y_val, y_val_pred_log))
r2_log   = r2_score(y_val, y_val_pred_log)

# 7) Invert back to original SalePrice before computing RMSLE
y_val_orig     = np.expm1(y_val)
y_val_pred_orig = np.expm1(y_val_pred_log)
rmsle = np.sqrt(mean_squared_log_error(y_val_orig, y_val_pred_orig))

print(f"Validation RMSE (log1p) : {rmse_log:.4f}")
print(f"Validation R² (log1p)   : {r2_log:.4f}")
print(f"Validation RMSLE        : {rmsle:.4f}")

# 8) Retrain on the full training set & predict on the test set
gbr.fit(X, y)
y_test_pred_log  = gbr.predict(X_test)
y_test_pred_orig = np.expm1(y_test_pred_log)

# 9) Create submission file
submission = pd.DataFrame({
    'Id': df_test['Id'],
    'SalePrice': y_test_pred_orig
})
submission.to_csv('submission.csv', index=False)

# 10) Preview
print(submission.head())

# after fitting gbr or rf:
import pandas as pd

# after fitting gbr or rf:
# Replace 'model' with the actual variable name of your trained model (e.g., gbr or rf)
importances = pd.Series(gbr.feature_importances_, index=features)
importances = importances.sort_values(ascending=False)
print(importances.head(0))

from sklearn.model_selection import train_test_split, RandomizedSearchCV
from sklearn.ensemble import GradientBoostingRegressor
from scipy.stats import randint, uniform

# 1) Assuming you already have X and y defined:
#    X = df_train[features].replace([np.inf, -np.inf], np.nan).fillna(0)
#    y = df_train['SalePrice']

# 2) Split into train/validation
X_train, X_val, y_train, y_val = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# 3) Define parameter distributions
param_dist = {
    'n_estimators': randint(200, 2000),
    'learning_rate': uniform(0.01, 0.2),
    'max_depth': randint(2, 8),
    'subsample': uniform(0.5, 0.5),
    'max_features': ['auto', 'sqrt', 'log2', None]
}

# 4) Set up and run RandomizedSearchCV
rs = RandomizedSearchCV(
    estimator=GradientBoostingRegressor(random_state=42),
    param_distributions=param_dist,
    n_iter=50,
    scoring='neg_mean_squared_log_error',
    cv=3,
    n_jobs=-1,
    random_state=42,
    verbose=2
)
rs.fit(X_train, y_train)

# 5) Output the best parameters
print("Optimized parameters:")
print(rs.best_params_)